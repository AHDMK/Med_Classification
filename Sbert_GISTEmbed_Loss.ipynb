{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just import from library if GISTEmbedLoss is available now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable, Dict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from sentence_transformers.models import Transformer\n",
    "\n",
    "\n",
    "class GISTEmbedLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SentenceTransformer,\n",
    "        guide: SentenceTransformer,\n",
    "        temperature: float = 0.01,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This loss is used to train a SentenceTransformer model using the GISTEmbed algorithm.\n",
    "        It takes a model and a guide model as input, and uses the guide model to guide the\n",
    "        in-batch negative sample selection. The cosine similarity is used to compute the loss\n",
    "        and the temperature parameter is used to scale the cosine similarities.\n",
    "\n",
    "        :param model: SentenceTransformer model based on a `transformers` model.\n",
    "        :param guide: SentenceTransformer model to guide the in-batch negative sample selection.\n",
    "        :param temperature: Temperature parameter to scale the cosine similarities.\n",
    "\n",
    "        References:\n",
    "            - For further details, see: https://arxiv.org/abs/2402.16829\n",
    "\n",
    "        Requirements:\n",
    "            1. (anchor, positive, negative) triplets\n",
    "            2. (anchor, positive) pairs\n",
    "\n",
    "        Relations:\n",
    "            - :class:`MultipleNegativesRankingLoss` is similar to this loss, but it does not use\n",
    "              a guide model to guide the in-batch negative sample selection. `GISTEmbedLoss` yields\n",
    "              a stronger training signal at the cost of some training overhead.\n",
    "\n",
    "        Inputs:\n",
    "            +---------------------------------------+--------+\n",
    "            | Texts                                 | Labels |\n",
    "            +=======================================+========+\n",
    "            | (anchor, positive, negative) triplets | none   |\n",
    "            +---------------------------------------+--------+\n",
    "            | (anchor, positive) pairs              | none   |\n",
    "            +---------------------------------------+--------+\n",
    "\n",
    "        Example:\n",
    "            ::\n",
    "\n",
    "                from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "                from torch.utils.data import DataLoader\n",
    "\n",
    "                model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                guide = SentenceTransformer('avsolatorio/GIST-small-Embedding-v0')\n",
    "                train_examples = [\n",
    "                    InputExample(texts=['The first query',  'The first positive passage',  'The first negative passage']),\n",
    "                    InputExample(texts=['The second query', 'The second positive passage', 'The second negative passage']),\n",
    "                ]\n",
    "                train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "                train_loss = losses.GISTEmbedLoss(model=model, guide=guide)\n",
    "                model.fit(\n",
    "                    [(train_dataloader, train_loss)],\n",
    "                    epochs=10,\n",
    "                )\n",
    "        \"\"\"\n",
    "        super(GISTEmbedLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.temperature = temperature\n",
    "        self.similarity_fct = nn.CosineSimilarity(dim=-1)\n",
    "        if not isinstance(model[0], Transformer) or not isinstance(guide[0], Transformer):\n",
    "            raise ValueError(\n",
    "                \"Both the training model and the guiding model must be based on the `transformers` architecture.\"\n",
    "            )\n",
    "        self.must_retokenize = (\n",
    "            model.tokenizer.vocab != guide.tokenizer.vocab or guide.max_seq_length < model.max_seq_length\n",
    "        )\n",
    "\n",
    "    def sim_matrix(self, embed1, embed2):\n",
    "        return self.similarity_fct(embed1.unsqueeze(1), embed2.unsqueeze(0))\n",
    "\n",
    "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
    "        embeddings = [self.model(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features]\n",
    "        with torch.no_grad():\n",
    "            if self.must_retokenize:\n",
    "                decoded = [\n",
    "                    self.model.tokenizer.batch_decode(sentence_feature[\"input_ids\"], skip_special_tokens=True)\n",
    "                    for sentence_feature in sentence_features\n",
    "                ]\n",
    "                sentence_features = [self.guide.tokenize(sentences) for sentences in decoded]\n",
    "                sentence_features = [\n",
    "                    {key: value.to(self.guide.device) for key, value in sentence_feature.items()}\n",
    "                    for sentence_feature in sentence_features\n",
    "                ]\n",
    "\n",
    "            guide_embeddings = [\n",
    "                self.guide(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features\n",
    "            ]\n",
    "\n",
    "        negative = None\n",
    "        negative_guide = None\n",
    "\n",
    "        if len(embeddings) == 2:\n",
    "            anchor, positive = embeddings\n",
    "            anchor_guide, positive_guide = guide_embeddings\n",
    "        elif len(embeddings) == 3:\n",
    "            anchor, positive, negative = embeddings\n",
    "            anchor_guide, positive_guide, negative_guide = guide_embeddings\n",
    "        else:\n",
    "            raise ValueError(\"Expected 2 or 3 embeddings, got {}\".format(len(embeddings)))\n",
    "\n",
    "        # Compute the model's similarities\n",
    "        ap_sim = self.sim_matrix(anchor, positive)\n",
    "        aa_sim = self.sim_matrix(anchor, anchor)\n",
    "        pp_sim = self.sim_matrix(positive, positive)\n",
    "\n",
    "        # Let's compute the similarity matrices for the combinations of anchor and positive samples.\n",
    "        guided_ap_sim = self.sim_matrix(anchor_guide, positive_guide)\n",
    "        guided_aa_sim = self.sim_matrix(anchor_guide, anchor_guide)\n",
    "        guided_pp_sim = self.sim_matrix(positive_guide, positive_guide)\n",
    "\n",
    "        # Define the anchor threshold\n",
    "        guided_sim = guided_ap_sim.diagonal().view(-1, 1)\n",
    "\n",
    "        # Find which samples cannot be used as negatives because they are\n",
    "        # more similar to the query than the assigned positive as deemed by the guide model.\n",
    "        # For these samples, we mask them with -inf to basically ignore their contribution to\n",
    "        # the loss.\n",
    "        ap_sim[guided_ap_sim > guided_sim] = -torch.inf\n",
    "        aa_sim[guided_aa_sim > guided_sim] = -torch.inf\n",
    "        pp_sim[guided_pp_sim > guided_sim] = -torch.inf\n",
    "\n",
    "        scores = [ap_sim, aa_sim, pp_sim]\n",
    "\n",
    "        # Handle the case where we have a negative sample\n",
    "        if negative is not None:\n",
    "            an_sim = self.sim_matrix(anchor, negative)\n",
    "            guided_an_sim = self.sim_matrix(anchor_guide, negative_guide)\n",
    "            an_sim[guided_an_sim > guided_sim] = -torch.inf\n",
    "\n",
    "            scores.append(an_sim)\n",
    "\n",
    "        scores = torch.cat(scores, dim=1) / self.temperature\n",
    "\n",
    "        # NOTE: We use arange here since the ap_sim matrix contains the anchor-positive\n",
    "        # similarities along the diagonal.\n",
    "        labels = torch.arange(scores.size(0)).long().to(scores.device)\n",
    "\n",
    "        return nn.CrossEntropyLoss()(scores, labels)\n",
    "\n",
    "    def get_config_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"guide\": self.guide,\n",
    "            \"temperature\": self.temperature,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "#### /print debug information to stdout\n",
    "\n",
    "model_name = 'dmis-lab/biobert-v1.1'\n",
    "train_batch_size = 16  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "# Save path of the model\n",
    "model_save_path = (\n",
    "    \"output/training_nli_v3_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "\n",
    "# Here we define our SentenceTransformer model\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# Check if dataset exists. If not, download and extract  it\n",
    "nli_dataset_path = \"data/AllNLI.tsv.gz\"\n",
    "sts_dataset_path = \"data/stsbenchmark.tsv.gz\"\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/AllNLI.tsv.gz\", nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/stsbenchmark.tsv.gz\", sts_dataset_path)\n",
    "\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "\n",
    "def add_to_samples(sent1, sent2, label):\n",
    "    if sent1 not in train_data:\n",
    "        train_data[sent1] = {\"contradiction\": set(), \"entailment\": set(), \"neutral\": set()}\n",
    "    train_data[sent1][label].add(sent2)\n",
    "\n",
    "\n",
    "train_data = {}\n",
    "with gzip.open(nli_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"train\":\n",
    "            sent1 = row[\"sentence1\"].strip()\n",
    "            sent2 = row[\"sentence2\"].strip()\n",
    "\n",
    "            add_to_samples(sent1, sent2, row[\"label\"])\n",
    "            add_to_samples(sent2, sent1, row[\"label\"])  # Also add the opposite\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "for sent1, others in train_data.items():\n",
    "    if len(others[\"entailment\"]) > 0 and len(others[\"contradiction\"]) > 0:\n",
    "        train_samples.append(\n",
    "            InputExample(\n",
    "                texts=[sent1, random.choice(list(others[\"entailment\"])), random.choice(list(others[\"contradiction\"]))]\n",
    "            )\n",
    "        )\n",
    "        train_samples.append(\n",
    "            InputExample(\n",
    "                texts=[random.choice(list(others[\"entailment\"])), sent1, random.choice(list(others[\"contradiction\"]))]\n",
    "            )\n",
    "        )\n",
    "\n",
    "logging.info(\"Train samples: {}\".format(len(train_samples)))\n",
    "\n",
    "\n",
    "# Special data loader that avoid duplicates within a batch\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_samples, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "# The guiding model\n",
    "guide_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our training loss\n",
    "train_loss = GISTEmbedLoss(model, guide_model)\n",
    "\n",
    "\n",
    "# Read STSbenchmark dataset and use it as development set\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"dev\":\n",
    "            score = float(row[\"score\"]) / 5.0  # Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))\n",
    "\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    dev_samples, batch_size=train_batch_size, name=\"sts-dev\"\n",
    ")\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=dev_evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=int(len(train_dataloader) * 0.1),\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    use_amp=False,  # Set to True, if your GPU supports FP16 operations\n",
    ")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "test_samples = []\n",
    "with gzip.open(sts_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"test\":\n",
    "            score = float(row[\"score\"]) / 5.0  # Normalize score to range 0 ... 1\n",
    "            test_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    test_samples, batch_size=train_batch_size, name=\"sts-test\"\n",
    ")\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_to_hub(\"\",token='')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
