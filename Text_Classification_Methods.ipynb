{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#read data\n",
    "data = pd.read_pickle('')\n",
    "# data = pd.read_table('')\n",
    "# data = pd.read_csv('')\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize , clean and simplify data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    sentence = \" \".join(mytokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenize'] = data['text'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence_transformers + non-deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util,losses\n",
    "model = SentenceTransformer('AHDMK/Sentence-GISTEmbedLoss-BioBert-Allnli-scinli') #use a suitable model for the task from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embeddings'] = data['text'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a gpu is available \n",
    "#list_data = [x for x in data['text'][:5000]]\n",
    "#x = model.encode(list)\n",
    "#save into dictionary then concat to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get classes\n",
    "classes=data['Category'].unique().tolist()\n",
    "nb_classes = len(classes)\n",
    "print(nb_classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn categories to indices \n",
    "for i,type_c in enumerate(classes):\n",
    "   for j,type_t in enumerate(data['Category']):\n",
    "       if type_c == type_t :\n",
    "           data.loc[j,'Category'] = i\n",
    "for i,type_c in enumerate(classes):\n",
    "   for j,type_t in enumerate(data_test['Category']):\n",
    "       if type_c == type_t :\n",
    "           data_test.loc[j,'Category'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = data['embeddings'].to_list()\n",
    "y_train = data['Category'].to_list()\n",
    "X_test = data_test['embeddings'].to_list()\n",
    "y_test = data_test['Category'].to_list()\n",
    "#if there is no test data use train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clfs = [\n",
    "    ('LogisticRegression', LogisticRegression(max_iter=3000,\n",
    "                                              class_weight='balanced')\n",
    "    ),\n",
    "    ('RandomForest', RandomForestClassifier(max_depth=18,\n",
    "                                            n_estimators=75,\n",
    "                                            random_state=0)\n",
    "    ),\n",
    "    ('KNN 5', KNeighborsClassifier(n_neighbors=5)\n",
    "    ),\n",
    "    ('SVM C1', SVC(C=1,\n",
    "                   class_weight='balanced')\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "def print_val_scores(scores: list[float]) -> None:\n",
    "\n",
    "  print(f'Cross validation scores: mean: {np.mean(scores):.3f}, '\n",
    "        f'all: {[round(score, 3) for score in scores]}')\n",
    "\n",
    "\n",
    "def print_stratified_kfold(clfs: list[tuple[str, any]], X_train: pd.DataFrame,\n",
    "                           y_train: pd.Series, n_splits: int = 5, cv: int = 5,\n",
    "                           ) -> None:\n",
    "\n",
    "  for clf in clfs:\n",
    "    print(f'\\nStratifiedKFold - classifier: {clf[0]}:\\n')\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    scores = cross_val_score(clf[1],\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=cv)\n",
    "\n",
    "    print_val_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "print_stratified_kfold(clfs, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "clf =  SVC(C=1,class_weight='balanced') #use the model with best accuracy from cross validation (or try all if the test data is not similar to training data)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# prob = clf.predict_proba(X_test)\n",
    "# print(prob)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(f'SVM - acc {accuracy:.3f}', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:int(len(data)*0.8)]\n",
    "validate = data[int(len(data)*0.8):]\n",
    "validate = validate.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from huggingface_hub import hf_hub_download\n",
    "Embedding = r\"/kaggle/input/biowordvec/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
    "#w2vmodel = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id=\"Word2vec/nlpl_222\", filename=\"model.bin\"), binary=True, unicode_errors=\"ignore\")\n",
    "BioWordVec = KeyedVectors.load_word2vec_format(Embedding,binary=True)\n",
    "weights = BioWordVec #pick a model suitable for the task , load it and set weights = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "\n",
    "max_words = 250\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['Category']\n",
    "        self.text = df['text']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        data = self.text[idx]\n",
    "        #sample = {\"text\": data, \"Category\": label}\n",
    "        X = tokenizer(data)\n",
    "        y=[0]*len(X)\n",
    "        for i,text in enumerate(X):\n",
    "            try:\n",
    "             y[i] = weights.key_to_index[text]\n",
    "            except : 0\n",
    "        y=y[:max_words]\n",
    "        G = [0]*max_words\n",
    "        for i in range(len(y)):\n",
    "            G[i]=y[i]\n",
    "        return torch.tensor(G, dtype=torch.int32), torch.tensor(label)\n",
    "    \n",
    "train_set = CustomTextDataset(train)\n",
    "validation_set = CustomTextDataset(validate)\n",
    "test_set = CustomTextDataset(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device = gpu if available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#2 gpus : \n",
    "#device_model = torch.device('cuda:0')\n",
    "#device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F \n",
    "\n",
    "k1 = max_words+1-3\n",
    "k2 = max_words+1-4\n",
    "k3 = max_words+1-5\n",
    "vector_len = 200\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        D = 300\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=weights.key_to_index['pad'])\n",
    "        #self.embed = nn.Embedding(199808, D)\n",
    "        #self.embed.weight.data.copy_(embedding_weights)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 100, kernel_size=(3,vector_len), stride=1,padding=0),  # h = 9-3 +1  and w = 1 output : 7x1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(k1,1), stride=1)) #1x1\n",
    "      \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 100, kernel_size=(4,vector_len), stride=1,padding=0), #6x1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(k2,1), stride=1))  #1x1\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(1, 100, kernel_size=(5,vector_len), stride=1,padding=0), #5x1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(k3,1), stride=1)) #1X1\n",
    "       \n",
    "        self.drop_out = nn.Dropout()\n",
    "        #concat operation\n",
    "        self.fc1 = nn.Linear(1 * 1 * 100 * 3, 100)\n",
    "        self.fc2 = nn.Linear(100, nb_classes)\n",
    "        \n",
    "        #self.fc3 = nn.Linear(100,3)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        #x=x.to(device)  #if the embedding layer is very large you can set it on a different gpu if available, move the data to it and then back after embedding\n",
    "        x = self.embed(x)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #x=x.to(device_model)\n",
    "        #print(x.shape)\n",
    "        x3 = self.layer1(x)\n",
    "        #print(x3.shape)\n",
    "        x4 = self.layer2(x)\n",
    "        x5 = self.layer3(x)\n",
    "        x3 = x3.reshape(x3.size(0), -1)\n",
    "        x4 = x4.reshape(x4.size(0), -1)\n",
    "        x5 = x5.reshape(x5.size(0), -1)\n",
    "        #print(x3.shape)\n",
    "        x3 = self.drop_out(x3)\n",
    "        x4 = self.drop_out(x4)\n",
    "        x5 = self.drop_out(x5)\n",
    "        out = torch.cat((x3,x4,x5),1)\n",
    "        #print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        #print(out.shape)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=4\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.to(device_model)\n",
    "#model.embed.to(device)\n",
    "def freeze_layer(layer):\n",
    " for param in layer.parameters():\n",
    "  param.requires_grad = False\n",
    "freeze_layer(model.embed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            X , Y = X.to(device_model), Y.to(device_model)\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.cpu().detach().numpy(), Y_preds.cpu().detach().numpy())))\n",
    "\n",
    "\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            #print('Y',Y)\n",
    "            X , Y = X.to(device_model), Y.to(device_model)\n",
    "            Y_preds = model(X) ## Make Predictions\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y) ## Calculate Loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad() ## Clear previously calculated gradients\n",
    "            loss.backward() ## Calculates Gradients\n",
    "            optimizer.step() ## Update network weights.\n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainModel(model, criterion, optimizer, train_loader, validation_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        X,Y = X.to(device_model) , Y.to(device_model)\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy()\n",
    "\n",
    "Y_actual, Y_preds = MakePredictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_actual, Y_preds, target_names=classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))\n",
    "ConfusionMatrixDisplay.from_predictions(Y_actual, Y_preds)\n",
    "plt.title(f'LSTM', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=''#trying the model give the input here\n",
    "X = tokenizer(text)\n",
    "print(X)\n",
    "y=[0]*len(X)\n",
    "\n",
    "for i,text in enumerate(X):\n",
    "    try:\n",
    "     y[i] = weights.key_to_index[text]\n",
    "    except : 1\n",
    "y=y[:max_words]\n",
    "G = [0]*max_words\n",
    "for i in range(len(y)):\n",
    "    G[i]=y[i]\n",
    "print(G)\n",
    "\n",
    "#G.to(device)\n",
    "G=[G]\n",
    "G=torch.tensor(G, dtype=torch.int32)\n",
    "G=G.to(device)\n",
    "#model2 = model2.to(device)\n",
    "output = model(G)\n",
    "print(output)\n",
    "v,i = max( (v,i) for i, v in enumerate(output[0]) )\n",
    "v = v.cpu().detach().numpy()\n",
    "print(\"Category :\" , classes[i],\"➜\",v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "embed_len = 200\n",
    "hidden_dim = 100\n",
    "n_layers=3\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        V = len(weights.key_to_index) + 1\n",
    "        D = 300\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=weights.key_to_index['pad'])\n",
    "        #self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        #self.embedding_layer = nn.Embedding(V, D)\n",
    "        #self.embedding_layer.weight.data.copy_(embedding_weights)\n",
    "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, nb_classes)  ## Input dimension are 2 times hidden dimensions due to bidirectional results\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(device)\n",
    "        embeddings = self.embedding(x)\n",
    "        #embeddings= embeddings.to(device_model)\n",
    "        hidden, carry = torch.randn(2*n_layers, len(x), hidden_dim), torch.randn(2*n_layers, len(x), hidden_dim)\n",
    "        hidden , carry = hidden.to(device_model) , carry.to(device_model)\n",
    "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_classifier = LSTMClassifier()\n",
    "lstm_classifier.to(device_model)\n",
    "#lstm_classifier.embedding.to(device)\n",
    "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "def freeze_layer(layer):\n",
    " for param in layer.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "freeze_layer(lstm_classifier.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainModel(lstm_classifier, criterion, optimizer, train_loader, validation_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_actual, Y_preds = MakePredictions(lstm_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_actual, Y_preds, target_names=classes))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_actual, Y_preds))\n",
    "ConfusionMatrixDisplay.from_predictions(Y_actual, Y_preds)\n",
    "plt.title(f'LSTM', size=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
