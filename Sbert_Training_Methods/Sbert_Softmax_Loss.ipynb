{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/kaggle/input/medical-text/train.dat',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "punctuations = '!\\\"#$%&\\'()*+,-/:;<=>?@[\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "    mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "    sentence = \" \".join(mytokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "def entailment(data):\n",
    "    data = data.iloc[:500]\n",
    "    data['cleaned'] = data['text'].apply(spacy_tokenizer)\n",
    "    data=data.assign(scombi=\"\")\n",
    "\n",
    "    for c in range(len(data)):\n",
    "        try:\n",
    "            sentence_tok = []\n",
    "            sentence_tok = tokenize.sent_tokenize(data[\"cleaned\"][c])\n",
    "            if sentence_tok[-1].endswith('.'):\n",
    "                sentence_tok =  sentence_tok\n",
    "            else:\n",
    "                sentence_tok[-1] = str(sentence_tok[-1]) + str('.')\n",
    "            data['scombi'][c] = '  '.join(sentence_tok)\n",
    "           \n",
    "        except:\n",
    "            print('1')\n",
    "    data = data.assign(pairs=\"\")    \n",
    "        \n",
    "    import itertools\n",
    "    for c in range(len(data)):\n",
    "        token = data.scombi[c].split('  ')\n",
    "        data.pairs[c] = list(itertools.combinations(token,2))\n",
    "\n",
    "    df_list_comp = pd.DataFrame()\n",
    "    test_list = [y for c in range(len(data.pairs)) for y in (data.pairs[c])]\n",
    "    df_list_comp = pd.DataFrame(test_list,columns = ['sentence1','sentence2'])\n",
    "    df_list_comp = df_list_comp.assign(label = 'entailment')\n",
    "    for i in range(len(df_list_comp)):\n",
    "        if len(df_list_comp.sentence1[i].split(' ')) < 5 or len(df_list_comp.sentence2[i].split(' ')) < 5 :\n",
    "            df_list_comp= df_list_comp.drop(i,axis=0)\n",
    "    df_list_comp.reset_index(drop=True)\n",
    "    return df_list_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def neutral(data):\n",
    "    data = data.iloc[:500]\n",
    "    data['cleaned'] = data['text'].apply(spacy_tokenizer)\n",
    "    data=data.assign(scombi=\"\")\n",
    "\n",
    "    for c in range(len(data)):\n",
    "        try:\n",
    "            sentence_tok = []\n",
    "            sentence_tok = tokenize.sent_tokenize(data[\"cleaned\"][c])\n",
    "            if sentence_tok[-1].endswith('.'):\n",
    "                sentence_tok =  sentence_tok\n",
    "            else:\n",
    "                sentence_tok[-1] = str(sentence_tok[-1]) + str('.')\n",
    "            data['scombi'][c] = '  '.join(sentence_tok)\n",
    "           \n",
    "        except:\n",
    "            print('1')\n",
    "    data1 = data.iloc[:250] \n",
    "    data2 = data.iloc[250:]  \n",
    "    data2 = data2.reset_index(drop=True)\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    # df1 = pd.DataFrame()\n",
    "    # df2 = pd.DataFrame()\n",
    "    for c in range(len(data1)):\n",
    "        token1 = data1.scombi[c].split('  ')\n",
    "        token2 = data2.scombi[c].split('  ')\n",
    "        list1.extend(token1)\n",
    "        list2.extend(token2)\n",
    "        # df1 = pd.DataFrame(list1,columns = ['single_sentences'])\n",
    "        # df2 = pd.DataFrame(list2,columns = ['single_sentences'])\n",
    "    x=0\n",
    "    for i in range(len(list1)):\n",
    "        if len(list1[i-x].split(' ')) < 5:\n",
    "            list1.pop(i-x)\n",
    "            x=x+1\n",
    "    x=0\n",
    "    for i in range(len(list2)):\n",
    "        if len(list2[i-x].split(' ')) < 5:\n",
    "            list2.pop(i-x)\n",
    "            x=x+1\n",
    "    # for i in range(len(df2)):\n",
    "    #     if len(df2.single_sentences[i].split(' ')) < 5:\n",
    "    #         df2= df2.drop(i,axis=0)\n",
    "    neutral_list = [(q1,q2) for q1 in list1 for q2 in random.sample(list2,k=13)]\n",
    "    df_neutral = pd.DataFrame(neutral_list,columns = ['sentence1','sentence2'])\n",
    "    df_neutral = df_neutral.assign(label='neutral')\n",
    "    return df_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def contradiction(data,data2):\n",
    "    data = data.iloc[:50]\n",
    "    data2 = data2.iloc[:50]\n",
    "    data['cleaned'] = data['text'].apply(spacy_tokenizer)\n",
    "    data2['cleaned'] = data2['text'].apply(spacy_tokenizer)\n",
    "    data=data.assign(scombi=\"\")\n",
    "    data2=data2.assign(scombi=\"\")\n",
    "\n",
    "    for c in range(len(data)):\n",
    "        try:\n",
    "            sentence_tok = []\n",
    "            sentence_tok2 = []\n",
    "            sentence_tok = tokenize.sent_tokenize(data[\"cleaned\"][c])\n",
    "            sentence_tok2 = tokenize.sent_tokenize(data2[\"cleaned\"][c])\n",
    "            if sentence_tok[-1].endswith('.'):\n",
    "                sentence_tok =  sentence_tok\n",
    "            else:\n",
    "                sentence_tok[-1] = str(sentence_tok[-1]) + str('.')\n",
    "            if sentence_tok2[-1].endswith('.'):\n",
    "                sentence_tok2 =  sentence_tok2\n",
    "            else:\n",
    "                sentence_tok2[-1] = str(sentence_tok2[-1]) + str('.')\n",
    "            data['scombi'][c] = '  '.join(sentence_tok)\n",
    "            data2['scombi'][c] = '  '.join(sentence_tok2)\n",
    "           \n",
    "        except:\n",
    "            print('1')\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    # df1 = pd.DataFrame()\n",
    "    # df2 = pd.DataFrame()\n",
    "    for c in range(len(data)):\n",
    "        token1 = data.scombi[c].split('  ')\n",
    "        token2 = data2.scombi[c].split('  ')\n",
    "        list1.extend(token1)\n",
    "        list2.extend(token2)\n",
    "        # df1 = pd.DataFrame(list1,columns = ['single_sentences'])\n",
    "        # df2 = pd.DataFrame(list2,columns = ['single_sentences'])\n",
    "    x=0\n",
    "    for i in range(len(list1)):\n",
    "        if len(list1[i-x].split(' ')) < 5:\n",
    "            list1.pop(i-x)\n",
    "            x=x+1\n",
    "    x=0\n",
    "    for i in range(len(list2)):\n",
    "        if len(list2[i-x].split(' ')) < 5:\n",
    "            list2.pop(i-x)\n",
    "            x=x+1\n",
    "    # for i in range(len(df2)):\n",
    "    #     if len(df2.single_sentences[i].split(' ')) < 5:\n",
    "    #         df2= df2.drop(i,axis=0)\n",
    "    contradiction_list = [(q1,q2) for q1 in list1 for q2 in random.sample(list2,k=35)]\n",
    "    df_contradiction = pd.DataFrame(contradiction_list,columns = ['sentence1','sentence2'])\n",
    "    df_contradiction = df_contradiction.assign(label='contradiction')\n",
    "    return df_contradiction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call for dataframes then concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lst = list(range(len(data)))\n",
    "random.seed(5)\n",
    "random_list = random.sample(lst,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(random_list)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.assign(split=\"train\")\n",
    "\n",
    "for z in random_list:\n",
    "    data.loc[z,'split'] = 'eval'\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.evaluation import LabelAccuracyEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\":1,\"neutral\":2}\n",
    "train_samples_my=[]\n",
    "dev_samples_my=[]\n",
    "for i in range(len(data)):\n",
    "    label_id = label2int[data['label'][i]]\n",
    "    if data['split'][i] == 'train':\n",
    "        train_samples_my.append(InputExample(texts=[data['sentence1'][i],data['sentence2'][i]],label=label_id))\n",
    "    else:\n",
    "        dev_samples_my.append(InputExample(texts=[data['sentence1'][i],data['sentence2'][i]],label=label_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_batch_size = 16\n",
    "model_save_path = './model'\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "#bert_bio = models.Transformer('dmis-lab/biobert-v1.1')\n",
    "#pooler = models.Pooling(bert_bio.get_word_embedding_dimension(),pooling_mode_mean_tokens = True)\n",
    "#model = SentenceTransformer(modules=[bert_bio, pooler])\n",
    "model= SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size=16\n",
    "loader = DataLoader(train_samples_my,batch_size,shuffle=True)\n",
    "dev_loader = DataLoader(dev_samples_my,batch_size,shuffle=True)\n",
    "\n",
    "from sentence_transformers import losses\n",
    "\n",
    "loss = losses.SoftmaxLoss(model=model,num_labels=3,sentence_embedding_dimension=384)\n",
    "evaluator_my = LabelAccuracyEvaluator(dev_loader, name=\"medical_eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "model.fit(train_objectives=[(loader, loss)],\n",
    "          \n",
    "          epochs = epochs,\n",
    "          warmup_steps = warmup_steps,\n",
    "      \n",
    "          output_path='./triple_loss_all_1000'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data_test = pd.read_csv('')\n",
    "data_test = data_test.drop_duplicates()\n",
    "data_test = data_test.reset_index(drop =True)\n",
    "\n",
    "\n",
    "classes=data['Category'].unique().tolist()\n",
    "nb_classes = len(classes)\n",
    "print(nb_classes)\n",
    "print(classes)\n",
    "\n",
    "\n",
    "for i,type_c in enumerate(classes):\n",
    "   for j,type_t in enumerate(data['Category']):\n",
    "       if type_c == type_t :\n",
    "           data.loc[j,'Category'] = i\n",
    "for i,type_c in enumerate(classes):\n",
    "   for j,type_t in enumerate(data_test['Category']):\n",
    "       if type_c == type_t :\n",
    "           data_test.loc[j,'Category'] = i\n",
    "           \n",
    "           \n",
    "data['embeddings'] = data['text'].apply(model.encode)\n",
    "data_test['embeddings'] = data_test['text'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train = data['embeddings'].to_list()\n",
    "y_train = data['Category'].to_list()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "X_test = data_test['embeddings'].to_list()\n",
    "y_test = data_test['Category'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clfs = [\n",
    "    ('LogisticRegression', LogisticRegression(max_iter=3000,\n",
    "                                              class_weight='balanced')\n",
    "    ),\n",
    "    ('RandomForest', RandomForestClassifier(max_depth=18,\n",
    "                                            n_estimators=75,\n",
    "                                            random_state=0)\n",
    "    ),\n",
    "    ('KNN 5', KNeighborsClassifier(n_neighbors=4)\n",
    "    ),\n",
    "    ('SVM C1', SVC(C=1,\n",
    "                   class_weight='balanced')\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "def print_val_scores(scores: list[float]) -> None:\n",
    "\n",
    "  print(f'Cross validation scores: mean: {np.mean(scores):.3f}, '\n",
    "        f'all: {[round(score, 3) for score in scores]}')\n",
    "\n",
    "\n",
    "def print_stratified_kfold(clfs: list[tuple[str, any]], X_train: pd.DataFrame,\n",
    "                           y_train: pd.Series, n_splits: int = 5, cv: int = 5,\n",
    "                           ) -> None:\n",
    "\n",
    "  for clf in clfs:\n",
    "    print(f'\\nStratifiedKFold - classifier: {clf[0]}:\\n')\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    scores = cross_val_score(clf[1],\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=cv)\n",
    "\n",
    "    print_val_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stratified_kfold(clfs, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "clf =  SVC(C=1,class_weight='balanced')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# prob = clf.predict_proba(X_test)\n",
    "# print(prob)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(f'SVM - acc {accuracy:.3f}', size=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
